{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import holidays\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data(file_path='../../public/data/pjm_dataset/pjm_hourly_est.csv'):\n",
    "    \"\"\"\n",
    "    Load and perform initial preprocessing of PJM data\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    df = pd.read_csv(file_path, parse_dates=['Datetime'])\n",
    "    \n",
    "    # Sort by datetime to ensure temporal order\n",
    "    df = df.sort_values('Datetime')\n",
    "    \n",
    "    # Filter date range to where we have most complete data (based on EDA)\n",
    "    df = df[(df['Datetime'] >= '2002-04-01') & (df['Datetime'] <= '2018-08-03')]\n",
    "    \n",
    "    # Focus on PJME and add PJMW as a feature\n",
    "    df = df[['Datetime', 'PJME', 'PJMW']]\n",
    "    \n",
    "    # Handle the one missing value in PJMW\n",
    "    df['PJMW'] = df['PJMW'].fillna(method='ffill')\n",
    "    \n",
    "    # Print data info for debugging\n",
    "    print(\"Initial data shape:\", df.shape)\n",
    "    print(\"\\nMissing values in initial data:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Create time-based features identified in EDA\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df['hour'] = df['Datetime'].dt.hour\n",
    "    df['day'] = df['Datetime'].dt.day\n",
    "    df['month'] = df['Datetime'].dt.month\n",
    "    df['year'] = df['Datetime'].dt.year\n",
    "    df['dayofweek'] = df['Datetime'].dt.dayofweek\n",
    "    df['quarter'] = df['Datetime'].dt.quarter\n",
    "    df['weekofyear'] = df['Datetime'].dt.isocalendar().week\n",
    "    \n",
    "    # Cyclical encoding of time features\n",
    "    # This helps capture the cyclical nature of time features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek']/7)\n",
    "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek']/7)\n",
    "    \n",
    "    # Is weekend feature\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Time of day categories (based on load patterns from EDA)\n",
    "    df['timeofday'] = pd.cut(df['hour'], \n",
    "                            bins=[-1, 6, 12, 17, 23],\n",
    "                            labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_holiday_features(df):\n",
    "    \"\"\"\n",
    "    Add holiday-related features\n",
    "    \"\"\"\n",
    "    us_holidays = holidays.US()\n",
    "    \n",
    "    df['is_holiday'] = df['Datetime'].dt.date.apply(\n",
    "        lambda x: 1 if x in us_holidays else 0\n",
    "    )\n",
    "    \n",
    "    # Add features for days before/after holidays\n",
    "    df['is_day_before_holiday'] = df['Datetime'].dt.date.apply(\n",
    "        lambda x: 1 if (x + timedelta(days=1)) in us_holidays else 0\n",
    "    )\n",
    "    df['is_day_after_holiday'] = df['Datetime'].dt.date.apply(\n",
    "        lambda x: 1 if (x - timedelta(days=1)) in us_holidays else 0\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lagged_features(df, target_col='PJME', lags=[1, 24, 48, 168]):\n",
    "    \"\"\"\n",
    "    Create lagged features based on target variable\n",
    "    lags: list of hour offsets for which to create lags\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'{target_col}_lag_{lag}h'] = df[target_col].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_rolling_features(df, target_col='PJME', \n",
    "                          windows=[24, 168], stats=['mean', 'std']):\n",
    "    \"\"\"\n",
    "    Create rolling window features\n",
    "    windows: list of hour windows for which to create features\n",
    "    stats: list of statistics to compute for each window\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for window in windows:\n",
    "        for stat in stats:\n",
    "            df[f'{target_col}_roll_{window}h_{stat}'] = getattr(\n",
    "                df[target_col].rolling(window, min_periods=1), stat\n",
    "            )()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in features with a more robust approach\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Handle temporal features (these shouldn't have missing values)\n",
    "    time_cols = ['hour', 'day', 'month', 'year', 'dayofweek', 'quarter', 'weekofyear',\n",
    "                'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', \n",
    "                'dayofweek_cos', 'is_weekend']\n",
    "    \n",
    "    # 2. Handle holiday features\n",
    "    holiday_cols = ['is_holiday', 'is_day_before_holiday', 'is_day_after_holiday']\n",
    "    df[holiday_cols] = df[holiday_cols].fillna(0)\n",
    "    \n",
    "    # 3. Handle lagged and rolling features\n",
    "    lag_roll_cols = [col for col in df.columns if 'lag' in col or 'roll' in col]\n",
    "    \n",
    "    # First forward fill\n",
    "    df[lag_roll_cols] = df[lag_roll_cols].fillna(method='ffill')\n",
    "    \n",
    "    # Then backward fill any remaining NAs\n",
    "    df[lag_roll_cols] = df[lag_roll_cols].fillna(method='bfill')\n",
    "    \n",
    "    # 4. For any remaining missing values in numeric columns, fill with median\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "            \n",
    "    # 5. For categorical columns, fill with mode\n",
    "    cat_cols = ['timeofday']\n",
    "    for col in cat_cols:\n",
    "        if col in df.columns and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def scale_features(df, exclude_cols=['Datetime', 'PJME', 'timeofday']):\n",
    "    \"\"\"\n",
    "    Scale numerical features\n",
    "    \"\"\"\n",
    "    # Identify columns to scale\n",
    "    cols_to_scale = [col for col in df.columns \n",
    "                    if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"\n",
    "    Main function to prepare all features\n",
    "    \"\"\"\n",
    "    # Create all feature groups\n",
    "    df = create_temporal_features(df)\n",
    "    df = create_holiday_features(df)\n",
    "    df = create_lagged_features(df)\n",
    "    df = create_rolling_features(df)\n",
    "    \n",
    "    # Handle missing values\n",
    "    df = handle_missing_values(df)\n",
    "    \n",
    "    # Scale features\n",
    "    df, scaler = scale_features(df)\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "def split_data(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets, respecting temporal order\n",
    "    \"\"\"\n",
    "    # Calculate split point\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    # Split the data\n",
    "    train = df.iloc[:split_idx]\n",
    "    test = df.iloc[split_idx:]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def validate_features(df_featured):\n",
    "    \"\"\"\n",
    "    Validate the engineered features\n",
    "    \"\"\"\n",
    "    # Check for missing values by column\n",
    "    missing = df_featured.isnull().sum()\n",
    "    print(\"\\nMissing values by column:\")\n",
    "    print(missing[missing > 0])\n",
    "    \n",
    "    # Check feature correlations with target, excluding non-numeric columns\n",
    "    numeric_df = df_featured.select_dtypes(include=['float64', 'int64'])\n",
    "    correlations = numeric_df.corr()['PJME'].sort_values(ascending=False)\n",
    "    print(\"\\nTop 10 feature correlations with PJME:\")\n",
    "    print(correlations[:10])\n",
    "    \n",
    "    # Check feature value ranges\n",
    "    print(\"\\nFeature ranges:\")\n",
    "    print(numeric_df.describe())\n",
    "    \n",
    "    return missing, correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (143207, 3)\n",
      "\n",
      "Missing values in initial data:\n",
      "Datetime    0\n",
      "PJME        0\n",
      "PJMW        1\n",
      "dtype: int64\n",
      "Dataset shape: (143207, 29)\n",
      "\n",
      "Features created: ['PJMW', 'hour', 'day', 'month', 'year', 'dayofweek', 'quarter', 'weekofyear', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'is_weekend', 'timeofday', 'is_holiday', 'is_day_before_holiday', 'is_day_after_holiday', 'PJME_lag_1h', 'PJME_lag_24h', 'PJME_lag_48h', 'PJME_lag_168h', 'PJME_roll_24h_mean', 'PJME_roll_24h_std', 'PJME_roll_168h_mean', 'PJME_roll_168h_std']\n",
      "\n",
      "Missing values: 0\n",
      "\n",
      "Missing values by column:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Top 10 feature correlations with PJME:\n",
      "PJME                   1.000000\n",
      "PJME_lag_1h            0.974913\n",
      "PJME_lag_24h           0.891863\n",
      "PJMW                   0.875714\n",
      "PJME_lag_168h          0.781288\n",
      "PJME_lag_48h           0.773443\n",
      "PJME_roll_24h_mean     0.702885\n",
      "PJME_roll_168h_mean    0.566898\n",
      "PJME_roll_24h_std      0.490923\n",
      "PJME_roll_168h_std     0.408539\n",
      "Name: PJME, dtype: float64\n",
      "\n",
      "Feature ranges:\n",
      "                PJME          PJMW      hour_sin      hour_cos     month_sin  \\\n",
      "count  143207.000000  1.432070e+05  1.432070e+05  1.432070e+05  1.432070e+05   \n",
      "mean    32110.726005  5.041034e-17  1.163506e-17 -4.507037e-17 -6.668455e-17   \n",
      "std      6488.198839  1.000003e+00  1.000003e+00  1.000003e+00  1.000003e+00   \n",
      "min     14544.000000 -5.224376e+00 -1.414036e+00 -1.414046e+00 -1.421294e+00   \n",
      "25%     27589.500000 -7.101920e-01 -9.998278e-01 -9.998269e-01 -7.101480e-01   \n",
      "50%     31437.000000 -7.391678e-02  1.582083e-04  1.871301e-04  9.978750e-04   \n",
      "75%     35706.000000  6.634680e-01  1.000144e+00  1.000201e+00  7.121438e-01   \n",
      "max     62009.000000  4.076681e+00  1.414352e+00  1.414420e+00  1.423290e+00   \n",
      "\n",
      "          month_cos  dayofweek_sin  dayofweek_cos    is_weekend    is_holiday  \\\n",
      "count  1.432070e+05   1.432070e+05   1.432070e+05  1.432070e+05  1.432070e+05   \n",
      "mean   4.763182e-17   1.147133e-16  -3.373921e-17 -6.598992e-17 -3.096068e-17   \n",
      "std    1.000003e+00   1.000003e+00   1.000003e+00  1.000003e+00  1.000003e+00   \n",
      "min   -1.380421e+00  -1.379553e+00  -1.274049e+00 -6.319656e-01 -1.681114e-01   \n",
      "25%   -1.191951e+00  -1.106457e+00  -1.274049e+00 -6.319656e-01 -1.681114e-01   \n",
      "50%    2.633909e-02  -7.122633e-04  -3.146374e-01 -6.319656e-01 -1.681114e-01   \n",
      "75%    7.297190e-01   1.105033e+00   8.817291e-01  1.582365e+00 -1.681114e-01   \n",
      "max    1.433099e+00   1.378129e+00   1.414162e+00  1.582365e+00  5.948436e+00   \n",
      "\n",
      "       is_day_before_holiday  is_day_after_holiday   PJME_lag_1h  \\\n",
      "count           1.432070e+05          1.432070e+05  1.432070e+05   \n",
      "mean           -2.341898e-17         -3.076222e-17  1.571850e-16   \n",
      "std             1.000003e+00          1.000003e+00  1.000003e+00   \n",
      "min            -1.681114e-01         -1.681114e-01 -2.707469e+00   \n",
      "25%            -1.681114e-01         -1.681114e-01 -6.969003e-01   \n",
      "50%            -1.681114e-01         -1.681114e-01 -1.038250e-01   \n",
      "75%            -1.681114e-01         -1.681114e-01  5.541372e-01   \n",
      "max             5.948436e+00          5.948436e+00  4.608103e+00   \n",
      "\n",
      "       PJME_lag_24h  PJME_lag_48h  PJME_lag_168h  PJME_roll_24h_mean  \\\n",
      "count  1.432070e+05  1.432070e+05   1.432070e+05        1.432070e+05   \n",
      "mean   3.969318e-17  2.460977e-16   1.428955e-17       -5.985732e-16   \n",
      "std    1.000003e+00  1.000003e+00   1.000003e+00        1.000003e+00   \n",
      "min   -2.707125e+00 -2.706831e+00  -2.704297e+00       -2.721616e+00   \n",
      "25%   -6.971216e-01 -6.971170e-01  -6.972945e-01       -7.444047e-01   \n",
      "50%   -1.037229e-01 -1.034587e-01  -1.028666e-01       -1.578471e-01   \n",
      "75%    5.544102e-01  5.543977e-01   5.544245e-01        6.229525e-01   \n",
      "max    4.608633e+00  4.609277e+00   4.608953e+00        4.284281e+00   \n",
      "\n",
      "       PJME_roll_24h_std  PJME_roll_168h_mean  PJME_roll_168h_std  \n",
      "count       1.432070e+05         1.432070e+05        1.432070e+05  \n",
      "mean        4.016950e-16         9.050046e-16        5.509414e-16  \n",
      "std         1.000003e+00         1.000003e+00        1.000003e+00  \n",
      "min        -2.063121e+00        -2.669411e+00       -2.451086e+00  \n",
      "25%        -6.975365e-01        -8.412530e-01       -7.448716e-01  \n",
      "50%        -3.087099e-01        -1.445707e-01       -4.569592e-01  \n",
      "75%         5.307237e-01         6.873063e-01        7.536372e-01  \n",
      "max         4.110073e+00         3.780336e+00        3.554695e+00  \n",
      "\n",
      "Train shape: (114565, 29)\n",
      "Test shape: (28642, 29)\n",
      "\n",
      "Processed data saved to CSV files\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = load_and_preprocess_data()\n",
    "\n",
    "# Prepare features\n",
    "df_featured, scaler = prepare_features(df)\n",
    "\n",
    "# Basic validation\n",
    "print(\"Dataset shape:\", df_featured.shape)\n",
    "print(\"\\nFeatures created:\", [col for col in df_featured.columns if col not in ['Datetime', 'PJME']])\n",
    "print(\"\\nMissing values:\", df_featured.isnull().sum().sum())\n",
    "\n",
    "# Validate features\n",
    "missing, correlations = validate_features(df_featured)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = split_data(df_featured)\n",
    "print(\"\\nTrain shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "    \n",
    "# Save processed data\n",
    "train_data.to_csv('../../data/processed/train_data.csv', index=False)\n",
    "test_data.to_csv('../../data/processed/test_data.csv', index=False)\n",
    "print(\"\\nProcessed data saved to CSV files\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
